{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":""},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h3 style=\"text-align: center;\"><b>Школа глубокого обучения ФПМИ МФТИ</b></h3>\n\n<h1 style=\"text-align: center;\"><b>Домашнее задание. Полносвязные и свёрточные нейронные сети</b></h1>","metadata":{"id":"yVymJesOkYa9"}},{"cell_type":"markdown","source":"\n\nВ этом занятии вам предстоит потренироваться построению нейронных сетей с помощью библиотеки Pytorch. Делать мы это будем на нескольких датасетах.\n\n\n\n\n\n\n","metadata":{"id":"_f6Y1AMgkYa_"}},{"cell_type":"code","source":"import numpy as np\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom torch.utils.data import TensorDataset, DataLoader\nimport random\n\nsns.set(style=\"darkgrid\", font_scale=1.4)","metadata":{"id":"Kd-KUTc1CBXm"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reduce stochasticity\n\nSEED = 42\n\n# PyTorch\ntorch.manual_seed(SEED)\n\n# CUDA (GPU)\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)  # For multi-GPU setups\n\n# NumPy\nnp.random.seed(SEED)\n\n# Python's built-in random\nrandom.seed(SEED)","metadata":{"id":"gpBil9_VkYbD"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.backends.cudnn.deterministic = True\ntorch.use_deterministic_algorithms(True)","metadata":{"id":"4BrUuZwSkYbE"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Часть 1. Датасет moons\n\nДавайте сгенерируем датасет и посмотрим на него!","metadata":{"id":"HvKrWYan4Rxr"}},{"cell_type":"code","source":"X, y = make_moons(n_samples=10000, random_state=42, noise=0.1)","metadata":{"id":"lo3nwjSJFUP7"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(16, 10))\nplt.title(\"Dataset\")\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"viridis\")\nplt.show()","metadata":{"id":"Xa9UCvRyJFqL","outputId":"be4f2504-114f-4061-8dfa-c67b6a402537"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Сделаем train/test split","metadata":{"id":"r6BcB14d4wGm"}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42)","metadata":{"id":"fv5MyTiCHjRh"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Загрузка данных\nВ PyTorch загрузка данных как правило происходит налету (иногда датасеты не помещаются в оперативную память). Для этого используются две сущности `Dataset` и `DataLoader`.\n\n1.   `Dataset` загружает каждый объект по отдельности.\n\n2.   `DataLoader` группирует объекты из `Dataset` в батчи.\n\nТак как наш датасет достаточно маленький мы будем использовать `TensorDataset`. Все, что нам нужно, это перевести из массива numpy в тензор с типом `torch.float32`.\n\n### Задание. Создайте тензоры с обучающими и тестовыми данными","metadata":{"id":"iTeLXQg240cQ"}},{"cell_type":"code","source":"X_train_t =  torch.FloatTensor(X_train)\ny_train_t =  torch.FloatTensor(y_train)\nX_val_t =  torch.FloatTensor(X_val)\ny_val_t =  torch.FloatTensor(y_val)","metadata":{"id":"msj3bXOeIcN-"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Создаем `Dataset` и `DataLoader`.","metadata":{"id":"gqwGMsE_7WBy"}},{"cell_type":"code","source":"train_dataset = TensorDataset(X_train_t, y_train_t)\nval_dataset = TensorDataset(X_val_t, y_val_t)\ntrain_dataloader = DataLoader(train_dataset, batch_size=128)\nval_dataloader = DataLoader(val_dataset, batch_size=128)","metadata":{"id":"ERvubjbMIu2J"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Logistic regression is my profession\n\n**Напоминание**\nДавайте вспоним, что происходит в логистической регрессии. На входе у нас есть матрица объект-признак X и столбец-вектор $y$ – метки из $\\{0, 1\\}$ для каждого объекта. Мы хотим найти такую матрицу весов $W$ и смещение $b$ (bias), что наша модель $XW + b$ будет каким-то образом предсказывать класс объекта. Как видно на выходе наша модель может выдавать число в интервале от $(-\\infty;\\infty)$. Этот выход как правило называют \"логитами\" (logits). Нам необходимо перевести его на интервал от $[0;1]$ для того, чтобы он выдавал нам вероятность принадлежности объекта к классу один, также лучше, чтобы эта функция была монотонной, быстро считалась, имела производную и на $-\\infty$ имела значение $0$, а на $+\\infty$ имела значение $1$. Такой класс функций называется сигмоидой. Чаще всего в качестве сигмоида берут:\n$$\n\\sigma(x) = \\frac{1}{1 + e^{-x}}.\n$$","metadata":{"id":"BT6eRdVNCO42"}},{"cell_type":"markdown","source":"### Задание. Реализация логистической регрессии\n\nВам необходимо написать модуль на PyTorch реализующий $logits = XW + b$, где $W$ и $b$ – параметры (`nn.Parameter`) модели. Иначе говоря, здесь мы реализуем своими руками модуль `nn.Linear` (в этом пункте его использование запрещено). Инициализируйте веса нормальным распределением (`torch.randn`).","metadata":{"id":"BKyYQcec7cu4"}},{"cell_type":"code","source":"class LinearRegression(nn.Module):\n    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n        super().__init__()\n        self.weights = nn.Parameter(torch.randn(in_features, out_features))\n        self.bias = bias\n        if bias:\n            self.bias_term = nn.Parameter(torch.randn(out_features))\n\n    def forward(self, x):\n        x = x @ self.weights\n        if self.bias:\n            x +=  self.bias_term\n        return x","metadata":{"id":"5U1y-0KtCTne"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"linear_regression = LinearRegression(2, 1)\nloss_function = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.SGD(linear_regression.parameters(), lr=0.05)","metadata":{"id":"jgIgTLHSM-10"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Вопрос 1.** Сколько обучаемых параметров у получившейся модели? Имеется в виду суммарное количество отдельных числовых переменных, а не количество тензоров.\n\n**Ответ:** Обучаемыми являются параметры матрицы весов W (их 2) и параментры вектора смещения bias (1)","metadata":{"id":"-VCVmee16yOl"}},{"cell_type":"code","source":"params = np.sum([p.numel() for p in linear_regression.parameters()])\nprint(params)","metadata":{"id":"VQuX6srUwUbQ","outputId":"033952ee-1a2f-442f-c5c4-76a643875b5b"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train loop\n\nПерейдём непосредственно к обучению.\n\nВот псевдокод, который поможет вам разобраться в том, что происходит во время обучения\n\n```python\nfor epoch in range(max_epochs):  # <----------- итерируемся по датасету несколько раз\n    for x_batch, y_batch in dataset:  # <------ итерируемся по датасету. Так как мы используем SGD а не GD, то берем батчи заданного размера\n        optimizer.zero_grad()  # <------------- обнуляем градиенты модели\n        outp = model(x_batch)  # <------------- получаем \"логиты\" из модели\n        loss = loss_func(outp, y_batch)  # <--- считаем \"лосс\" для логистической регрессии\n        loss.backward()  # <------------------- считаем градиенты\n        optimizer.step()  # <------------------ делаем шаг градиентного спуска\n        if convergence:  # <------------------- в случае сходимости выходим из цикла\n            break\n```\n\nВ коде ниже добавлено логирование `accuracy` и `loss`.","metadata":{"id":"4NV-JrNCoP8E"}},{"cell_type":"markdown","source":"### Задание. Реализация цикла обучения","metadata":{"id":"U1DvfCPY7TMZ"}},{"cell_type":"code","source":"tol = 1e-3\nlosses = []\nmax_epochs = 100\nprev_weights = torch.zeros_like(linear_regression.weights)\nstop_it = False\nfor epoch in range(max_epochs):\n    for it, (X_batch, y_batch) in enumerate(train_dataloader):\n        optimizer.zero_grad()\n        outp =  linear_regression(X_batch) # YOUR CODE. Use linear_regression to get outputs\n        loss =  loss_function(outp.squeeze() , y_batch) # YOUR CODE. Compute loss\n        loss.backward()\n        losses.append(loss.detach().flatten()[0])\n        optimizer.step()\n        probabilities = torch.sigmoid(outp) # YOUR CODE. Compute probabilities\n        preds = (probabilities > 0.5).type(torch.long)\n        batch_acc = (preds.flatten() == y_batch).type(torch.float32).sum() / y_batch.size(0)\n\n        if (it + epoch * len(train_dataloader)) % 100 == 0:\n            print(f\"Iteration: {it + epoch * len(train_dataloader)}\\nBatch accuracy: {batch_acc}\")\n        current_weights = linear_regression.weights.detach().clone()\n        if (prev_weights - current_weights).abs().max() < tol:\n            print(f\"\\nIteration: {it + epoch * len(train_dataloader)}.Convergence. Stopping iterations.\")\n            stop_it = True\n            break\n        prev_weights = current_weights\n    if stop_it:\n        break","metadata":{"id":"gVhlIfK0L93s","outputId":"b88bd678-932c-44f7-a015-14caf5798ed8"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Вопрос 2.** Сколько итераций потребовалось, чтобы алгоритм сошелся? (ваш ответ может варьироваться в зависимости от запуска)\n\n**Ответ:** на 300-ой из 755 итераций accuracy стал масимальным, следовательно, на 300 итерации алогритм сошёлся","metadata":{"id":"QBJ64-MT63_r"}},{"cell_type":"markdown","source":"### Визуализируем результаты","metadata":{"id":"As-pgFbzFmiI"}},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nplt.plot(range(len(losses)), losses)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.show()","metadata":{"id":"HzPRB8j4b1IF","outputId":"aee5cb1d-fffb-48b0-907b-3d8281428966"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nsns.set(style=\"white\")\n\nxx, yy = np.mgrid[-1.5:2.5:.01, -1.:1.5:.01]\ngrid = np.c_[xx.ravel(), yy.ravel()]\nbatch = torch.from_numpy(grid).type(torch.float32)\nwith torch.no_grad():\n    probs = torch.sigmoid(linear_regression(batch).reshape(xx.shape))\n    probs = probs.numpy().reshape(xx.shape)\n\nf, ax = plt.subplots(figsize=(16, 10))\nax.set_title(\"Decision boundary\", fontsize=14)\ncontour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\",\n                      vmin=0, vmax=1)\nax_c = f.colorbar(contour)\nax_c.set_label(\"$P(y = 1)$\")\nax_c.set_ticks([0, .25, .5, .75, 1])\n\nax.scatter(X[100:,0], X[100:, 1], c=y[100:], s=50,\n           cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n           edgecolor=\"white\", linewidth=1)\n\nax.set(xlabel=\"$X_1$\", ylabel=\"$X_2$\")\nplt.show()","metadata":{"id":"9cGgcBMNX2WP","outputId":"65b568c4-22cf-4f0c-8b27-7401700d2590"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Задание. Реализуйте predict и посчитайте accuracy на test.","metadata":{"id":"M9G-UkrE-Arr"}},{"cell_type":"code","source":"@torch.no_grad()\ndef predict(dataloader, model):\n    model.eval()\n    predictions = np.array([])\n    for x_batch, _ in dataloader:\n        outp = model(x_batch)\n        probabilities = torch.sigmoid(outp) # YOUR CODE. Compute probabilities\n        preds = (probabilities > 0.5).type(torch.long)\n        predictions = np.hstack((predictions, preds.numpy().flatten()))\n    return predictions.flatten()","metadata":{"id":"FP_imFpe7Ac4"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n# YOUR CODE. Compute total accuracy\naccuracy_score(y_val_t, predict(val_dataloader, linear_regression))","metadata":{"id":"v0vaJdAS7Dfq","outputId":"2d5df196-f6dd-42d6-91c4-fe302f21d744"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"id":"CMblPkvTZyIJ"}},{"cell_type":"markdown","source":"**Вопрос 3**\n\nКакое `accuracy` получается после обучения? (ваш ответ может варьироваться в зависимости от запуска)\n\n**Ответ:** 0.862\n","metadata":{"id":"s7TXs3kk7Kmq"}},{"cell_type":"markdown","source":"# Часть 2. Датасет MNIST\nДатасет MNIST содержит рукописные цифры. Загрузим датасет и создадим DataLoader-ы. Пример можно найти в семинаре по полносвязным нейронным сетям.","metadata":{"id":"cyivMZBZC0Ha"}},{"cell_type":"code","source":"import os\nfrom torchvision.datasets import MNIST\nfrom torchvision import transforms as tfs\n\n\ndata_tfs = tfs.Compose([\n    tfs.ToTensor(),\n    tfs.Normalize((0.5), (0.5))\n])\n\n# install for train and test\nroot = './'\ntrain_dataset = MNIST(root, train=True,  transform=data_tfs, download=True)\nval_dataset  = MNIST(root, train=False, transform=data_tfs, download=True)\n\ntrain_dataloader =  DataLoader(train_dataset, batch_size=64, shuffle=True)\nvalid_dataloader =  DataLoader(train_dataset, batch_size=64, shuffle=False)","metadata":{"id":"LuUE2wihC4GW","outputId":"b235566a-24e8-4bb2-cfd0-fe01e0a86dff"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Часть 2.1. Полносвязные нейронные сети\nСначала решим MNIST с помощью полносвязной нейронной сети.","metadata":{"id":"4t2w2XtSB1Hd"}},{"cell_type":"markdown","source":"### Задание. Простая полносвязная нейронная сеть\n\nСоздайте полносвязную нейронную сеть с помощью класса Sequential. Сеть состоит из:\n* Уплощения матрицы в вектор (nn.Flatten);\n* Двух скрытых слоёв из 128 нейронов с активацией nn.ELU;\n* Выходного слоя с 10 нейронами.\n","metadata":{"id":"HMtCBdCA-4bj"}},{"cell_type":"code","source":"activation = nn.ELU()\n\nmodel = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(28 * 28, 128),\n    activation,\n    nn.Linear(128, 128),\n    activation,\n    nn.Linear(128, 10)\n)","metadata":{"id":"ulxWHddEiQCr"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Задайте лосс для обучения (кросс-энтропия).\n\nВспомним, что такое кросс-энтропийная функция потерь:\n\n$$\nL = -\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{i=1}^{C} y_{ni} \\log(\\hat{y}_{ni})\n$$\n\nгде:\n- $N$ — количество объектов в выборке,\n- $C$ — количество классов,\n- $y_{ni}$ — истинная метка (one-hot представление),\n- $\\hat{y}_{ni}$ — предсказанная вероятность модели для класса $i$ на примере $n$.","metadata":{"id":"RWa4VK55ifMq"}},{"cell_type":"markdown","source":"***Пример расчета***\n\nРассмотрим пример расчета кросс-энтропийной функции потерь для задачи многоклассовой классификации с 10 классами. Предположим, что у нас есть один объект, истинный класс которого известен, а модель выдала свои предсказания в виде вероятностей для каждого класса.\n\nПусть объект является цифрой 2. Тогда:\n     $$\n     y = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n     $$\n\nПусть модель выдала следующие вероятности для 10 классов:  \n     $$\n     \\hat{y} = [0.1, 0.05, 0.6, 0.15, 0.05, 0.02, 0.01, 0.01, 0.01, 0.0]\n     $$\n\n$$\nL = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\n$$\n\n\n\nТолько один элемент $y_i$ равен 1, остальные равны 0. Поэтому:\n\n$\nL = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i) = - y_2 \\cdot log(\\hat{y}_2) = - 1 \\cdot log(0.6) \\approx 0.51\n$","metadata":{"id":"4rWTcTwVig4u"}},{"cell_type":"code","source":"criterion = torch.nn.CrossEntropyLoss() #YOUR CODE. Select a loss function\noptimizer = torch.optim.Adam(model.parameters())\n\nloaders = {\"train\": train_dataloader, \"valid\": valid_dataloader}","metadata":{"id":"FIl6z-AfivcK"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"id":"whvqhLjYmpKc"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train loop (seriously)\n\nДавайте разберемся с кодом ниже, который подойдет для 90% задач в будущем.\n\n\n```python\nfor epoch in range(max_epochs):  # <--------------- итерируемся по датасету несколько раз\n    for k, dataloader in loaders.items():  # <----- несколько dataloader для train / valid / test\n        for x_batch, y_batch in dataloader:  # <--- итерируемся по датасету. Так как мы используем SGD а не GD, то берем батчи заданного размера\n            if k == \"train\":\n                model.train()  # <------------------ переводим модель в режим train\n                optimizer.zero_grad()  # <--------- обнуляем градиенты модели\n                outp = model(x_batch)\n                loss = criterion(outp, y_batch) # <-считаем \"лосс\" для логистической регрессии\n                loss.backward()  # <--------------- считаем градиенты\n                optimizer.step()  # <-------------- делаем шаг градиентного спуска\n            else:  # <----------------------------- test/eval\n                model.eval()  # <------------------ переводим модель в режим eval\n                with torch.no_grad():  # <--------- НЕ считаем градиенты\n                    outp = model(x_batch)  # <------------- получаем \"логиты\" из модели\n            count_metrics(outp, y_batch)  # <-------------- считаем метрики\n```","metadata":{"id":"4Xl_HawRGRAe"}},{"cell_type":"markdown","source":"### Задание. Дополните цикл обучения.","metadata":{"id":"raKQWwQm_9Ff"}},{"cell_type":"code","source":"max_epochs = 10\naccuracy = {\"train\": [], \"valid\": []}\nfor epoch in range(max_epochs):\n    for k, dataloader in loaders.items():\n        epoch_correct = 0\n        epoch_all = 0\n        for x_batch, y_batch in dataloader:\n            if k == \"train\":\n                 model.train()\n                 optimizer.zero_grad()\n                 outp = model(x_batch)\n            else:\n                 model.eval()\n                 with torch.no_grad():\n                    outp = model(x_batch)\n\n            preds = outp.argmax(-1)\n            correct = (preds == y_batch).sum()\n            all = y_batch.size(0)\n            epoch_correct += correct.item()\n            epoch_all += all\n            if k == \"train\":\n                loss = criterion(outp, y_batch)\n                loss.backward()\n                optimizer.step()\n        if k == \"train\":\n            print(f\"Epoch: {epoch+1}\")\n\n        print(f\"Loader: {k}. Accuracy: {epoch_correct/epoch_all}\")\n        accuracy[k].append(epoch_correct/epoch_all)\n","metadata":{"id":"3Tmo1lsHjBE6","outputId":"4d897101-ea17-47b6-9728-e586a9811e27"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Задание. Протестируйте разные функции активации.\nПопробуйте разные функции активации. Для каждой функции активации посчитайте массив validation accuracy. Лучше реализовать это в виде функции, берущей на вход активацию и получающей массив из accuracies.","metadata":{"id":"fFRxO1-FK9U9"}},{"cell_type":"markdown","source":"Функции активации добавляют ***нелинейность в модель***, что позволяет сети решать сложные задачи, такие как классификация и регрессия. Без функций активации нейронная сеть была бы линейной моделью, так как ***композиция линейных преобразований*** это ***линейное преобразование***.\n\n**ReLU (Rectified Linear Unit)**\n   - Формула: $ f(x) = \\max(0, x) $\n   - Диапазон значений: $ [0, \\infty) $\n\n**Leaky ReLU**\n   - Формула: $ f(x) = \\max(\\alpha x, x) $, где $ \\alpha $ — маленькое положительное число (обычно $ \\alpha = 0.01 $).\n   - Диапазон значений: $ (-\\infty, \\infty) $\n\n**Exponential Linear Unit (ELU)**\n   - Формула:\n   $f(x) = $\n     \\begin{cases}\n     x & \\text{если } x > 0 \\\\\n     \\alpha (e^x - 1) & \\text{если } x \\leq 0\n     \\end{cases}\n   - Диапазон значений:  $ (-\\alpha, \\infty) $\n\n**Линейная функция активации**\n   - Формула: $ f(x) = x $\n   - Диапазон значений: $ (-\\infty, \\infty) $\n\n   Обратите внимание, что данная функция активации **не добавляет** нелинейности.","metadata":{"id":"SZbu5Yv1i-B4"}},{"cell_type":"code","source":"elu_accuracy = accuracy[\"valid\"]","metadata":{"id":"SezseDhEqhm2"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# YOUR CODE. Do the same thing with other activations (it's better to wrap into a function that returns a list of accuracies)\n\ndef test_activation_function(activation):\n\n  model = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(28 * 28, 128),\n    activation,\n    nn.Linear(128, 128),\n    activation,\n    nn.Linear(128, 10)\n    )\n\n  criterion = torch.nn.CrossEntropyLoss()\n  optimizer = torch.optim.Adam(model.parameters())\n  loaders = {\"train\": train_dataloader, \"valid\": valid_dataloader}\n\n  accuracy = {\"train\": [], \"valid\": []}\n\n  for epoch in range(max_epochs):\n      for k, dataloader in loaders.items():\n          epoch_correct = 0\n          epoch_all = 0\n          for x_batch, y_batch in dataloader:\n              if k == \"train\":\n                  model.train()\n                  optimizer.zero_grad()\n                  outp = model(x_batch)\n              else:\n                  model.eval()\n                  with torch.no_grad():\n                      outp = model(x_batch)\n\n              preds = outp.argmax(-1)\n              correct = (preds == y_batch).sum()\n              all = y_batch.size(0)\n              epoch_correct += correct.item()\n              epoch_all += all\n              if k == \"train\":\n                  loss = criterion(outp, y_batch)\n                  loss.backward()\n                  optimizer.step()\n\n          accuracy[k].append(epoch_correct/epoch_all)\n\n  return accuracy['valid']","metadata":{"id":"lSVBlHtuAUAh"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plain_accuracy = test_activation_function(nn.Identity()) #There is no non-linearity","metadata":{"id":"a1iXow0Tqsri"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"relu_accuracy = test_activation_function(nn.ReLU())","metadata":{"id":"yTdOnIsj_8gV"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"leaky_relu_accuracy = test_activation_function(nn.LeakyReLU())","metadata":{"id":"5M3WVqNW_918"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"ELU activation accuracy: {elu_accuracy[-1]}\")\nprint(f\"No activation accuracy: {plain_accuracy[-1]}\")\nprint(f\"ReLU activation accuracy: {relu_accuracy[-1]}\")\nprint(f\"LeakyReLU activation accuracy: {leaky_relu_accuracy [-1]}\")","metadata":{"id":"dLqx0b3qH4tQ","outputId":"10014c38-1234-4318-f25c-6b1dad9ff0eb"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Accuracy\nПостроим график accuracy/epoch для каждой функции активации.","metadata":{"id":"FK07mms3FwNd"}},{"cell_type":"code","source":"sns.set(style=\"darkgrid\", font_scale=1.4)\n\nplt.figure(figsize=(16, 10))\nplt.title(\"Valid accuracy\")\nplt.plot(range(max_epochs), plain_accuracy, label=\"No activation\", linewidth=2)\nplt.plot(range(max_epochs), relu_accuracy, label=\"ReLU activation\", linewidth=2)\nplt.plot(range(max_epochs), leaky_relu_accuracy, label=\"LeakyReLU activation\", linewidth=2)\nplt.plot(range(max_epochs), elu_accuracy, label=\"ELU activation\", linewidth=2)\nplt.legend()\nplt.xlabel(\"Epoch\")\nplt.show()","metadata":{"id":"cvKclpM5r-P3","outputId":"43504289-8d98-4774-f2b0-1e18864c400c"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(16, 10))\nplt.title(\"Valid accuracy\")\nplt.plot(range(max_epochs), relu_accuracy, label=\"ReLU activation\", linewidth=2)\nplt.plot(range(max_epochs), leaky_relu_accuracy, label=\"LeakyReLU activation\", linewidth=2)\nplt.plot(range(max_epochs), elu_accuracy, label=\"ELU activation\", linewidth=2)\nplt.legend()\nplt.xlabel(\"Epoch\")\nplt.show()","metadata":{"id":"4R8L1JCts_qz","outputId":"e0e63fc1-f4a7-4a02-d4a0-1f1b26919117"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Вопрос 4.** Какая из активаций показала наивысший `accuracy` к концу обучения?\n\n**Ответ:** ReLU","metadata":{"id":"bl8qYSSa7c-r"}},{"cell_type":"markdown","source":"## Часть 2.2 Сверточные нейронные сети","metadata":{"id":"ebq7icV8CbrU"}},{"cell_type":"markdown","source":"### Ядра\nСначала немного поработам с самим понятием ядра свёртки.","metadata":{"id":"9bXylTAuCqu-"}},{"cell_type":"code","source":"!wget https://img.the-village.kz/the-village.com.kz/post-cover/5x5-I6oiwjmq79dMCZMEbA-default.jpg -O sample_photo.jpg","metadata":{"id":"G7P75exc0acy","outputId":"e75b84de-1aed-4c32-eb26-aec915ed40af"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nsns.set(style=\"white\")\nimg = cv2.imread(\"sample_photo.jpg\")\nRGB_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(12, 8))\nplt.imshow(RGB_img)\nplt.show()","metadata":{"id":"88YOXq0CCyUM","outputId":"a9547687-574b-4257-f381-b256d8b88a6d"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Попробуйте посмотреть как различные свертки влияют на фото. Например, попробуйте\nA)\n```\n[0, 0, 0],\n[0, 1, 0],\n[0, 0, 0]\n```\nБ)\n```\n[0, 1, 0],\n[0, -2, 0],\n[0, 1, 0]\n```\nВ)\n```\n[0, 0, 0],\n[1, -2, 1],\n[0, 0, 0]\n```\nГ)\n```\n[0, 1, 0],\n[1, -4, 1],\n[0, 1, 0]\n```\nД)\n```\n[0, -1, 0],\n[-1, 5, -1],\n[0, -1, 0]\n```\nЕ)\n```\n[0.0625, 0.125, 0.0625],\n[0.125, 0.25, 0.125],\n[0.0625, 0.125, 0.0625]\n```\n\nНе стесняйтесь пробовать свои варианты!","metadata":{"id":"Iu9oxwbL-6xE"}},{"cell_type":"code","source":"conv_1 = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]]) # A\nconv_2 = np.array([[0, 1, 0], [0, -2, 0], [0, 1, 0]]) # Б\nconv_3 = np.array([[0, 0, 0], [1, -2, 1], [0, 0, 0]]) # В\nconv_4 = np.array([[0, 1, 0], [1, -4, 1], [0, 1, 0]]) # Г\nconv_5 = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]]) # Д\nconv_6 = np.array([[0.0625, 0.125, 0.0625], [0.125, 0.25, 0.125], [0.0625, 0.125, 0.0625]]) # E","metadata":{"id":"Xk2-TBuZjePS"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"А)","metadata":{"id":"z0mm6ingLiSV"}},{"cell_type":"code","source":"img_t = torch.from_numpy(RGB_img).type(torch.float32).unsqueeze(0)\nkernel = torch.tensor(conv_1).reshape(1, 1, 3, 3).type(torch.float32)\n\nkernel = kernel.repeat(3, 3, 1, 1)\nimg_t = img_t.permute(0, 3, 1, 2)  # [BS, H, W, C] -> [BS, C, H, W]\nimg_t = nn.ReflectionPad2d(1)(img_t)  # Pad Image for same output size\n\nresult = F.conv2d(img_t, kernel)[0]  #","metadata":{"id":"nFySLV_oLgJm"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Б)","metadata":{"id":"f5lV17hbLnId"}},{"cell_type":"code","source":"img_t = torch.from_numpy(RGB_img).type(torch.float32).unsqueeze(0)\nkernel = torch.tensor(conv_2).reshape(1, 1, 3, 3).type(torch.float32)\n\nkernel = kernel.repeat(3, 3, 1, 1)\nimg_t = img_t.permute(0, 3, 1, 2)  # [BS, H, W, C] -> [BS, C, H, W]\nimg_t = nn.ReflectionPad2d(1)(img_t)  # Pad Image for same output size\n\nresult = F.conv2d(img_t, kernel)[0]  #","metadata":{"id":"hk1MdBROLor-"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Г)","metadata":{"id":"5Puca8x5MBEN"}},{"cell_type":"code","source":"img_t = torch.from_numpy(RGB_img).type(torch.float32).unsqueeze(0)\nkernel = torch.tensor(conv_4).reshape(1, 1, 3, 3).type(torch.float32)\n\nkernel = kernel.repeat(3, 3, 1, 1)\nimg_t = img_t.permute(0, 3, 1, 2)  # [BS, H, W, C] -> [BS, C, H, W]\nimg_t = nn.ReflectionPad2d(1)(img_t)  # Pad Image for same output size\n\nresult = F.conv2d(img_t, kernel)[0]  #","metadata":{"id":"8X5NrctAMDVH"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Д)","metadata":{"id":"uQPw8RngLu5u"}},{"cell_type":"code","source":"img_t = torch.from_numpy(RGB_img).type(torch.float32).unsqueeze(0)\nkernel = torch.tensor(conv_5).reshape(1, 1, 3, 3).type(torch.float32)\n\nkernel = kernel.repeat(3, 3, 1, 1)\nimg_t = img_t.permute(0, 3, 1, 2)  # [BS, H, W, C] -> [BS, C, H, W]\nimg_t = nn.ReflectionPad2d(1)(img_t)  # Pad Image for same output size\n\nresult = F.conv2d(img_t, kernel)[0]  #","metadata":{"id":"_D9m8yUiLwrF"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Е)","metadata":{"id":"L-Uq9HrnLbV-"}},{"cell_type":"code","source":"img_t = torch.from_numpy(RGB_img).type(torch.float32).unsqueeze(0)\nkernel = torch.tensor(conv_6).reshape(1, 1, 3, 3).type(torch.float32)\n\nkernel = kernel.repeat(3, 3, 1, 1)\nimg_t = img_t.permute(0, 3, 1, 2)  # [BS, H, W, C] -> [BS, C, H, W]\nimg_t = nn.ReflectionPad2d(1)(img_t)  # Pad Image for same output size\n\nresult = F.conv2d(img_t, kernel)[0]  #","metadata":{"id":"yAYbsQOVjgUu"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"В)","metadata":{"id":"qp35MWSrLTtO"}},{"cell_type":"code","source":"img_t = torch.from_numpy(RGB_img).type(torch.float32).unsqueeze(0)\nkernel = torch.tensor([\n    [0, 0, 0],\n    [1, -2, 1],\n    [0, 0, 0]\n]).reshape(1, 1, 3, 3).type(torch.float32)\n\nkernel = kernel.repeat(3, 3, 1, 1)\nimg_t = img_t.permute(0, 3, 1, 2)  # [BS, H, W, C] -> [BS, C, H, W]\nimg_t = nn.ReflectionPad2d(1)(img_t)  # Pad Image for same output size\n\nresult = F.conv2d(img_t, kernel)[0]  #","metadata":{"id":"-0WNZro01pZE"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nresult_np = result.permute(1, 2, 0).numpy() / 256 / 3\n\nplt.imshow(result_np)\nplt.show()","metadata":{"id":"C-SAfc7x28ko","outputId":"6a562fe9-ea99-487f-edb9-ed7a898bc863"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Вопрос 5.** Как можно описать действия ядер, приведенных выше? Сопоставьте для каждой буквы число.\n\n1) Размытие\n\n2) Увеличение резкости\n\n3) Тождественное преобразование\n\n4) Выделение вертикальных границ\n\n5) Выделение горизонтальных границ\n\n6) Выделение границ\n\n**Ответ:**\n\nА - 3\n\nБ - 5\n\nВ - 4\n\nГ - 6\n\nД - 2\n\nЕ - 1","metadata":{"id":"oDapwqTXGC4e"}},{"cell_type":"markdown","source":"### Задание. Реализуйте LeNet\n\nЕсли мы сделаем параметры сверток обучаемыми, то можем добиться хороших результатов для задач компьютерного зрения. Реализуйте архитектуру LeNet, предложенную еще в 1998 году!\nНа этот раз используйте модульную структуру (без помощи класса Sequential).\n\nНаша нейронная сеть будет состоять из\n* Свёртки 3x3 (1 карта на входе, 6 на выходе) с активацией ReLU;\n* MaxPooling-а 2x2;\n* Свёртки 3x3 (6 карт на входе, 16 на выходе) с активацией ReLU;\n* MaxPooling-а 2x2;\n* Уплощения (nn.Flatten);\n* Полносвязного слоя со 120 нейронами и активацией ReLU;\n* Полносвязного слоя с 84 нейронами и активацией ReLU;\n* Выходного слоя из 10 нейронов.\n\n\n","metadata":{"id":"4b9ib9saC9Vb"}},{"cell_type":"code","source":"class LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        # 28x28 - начальный р-р изобр-я\n        self.conv1 = nn.Conv2d(1, 6, 3) # 28 - 3 + 1 = 26 -> 26x26\n        self.pool1 = nn.MaxPool2d(2, 2) # 13x13\n        self.conv2 = nn.Conv2d(6, 16, 3) # 11x11\n        self.pool2 = nn.MaxPool2d(2, 2) #5x5\n        self.fc1 = nn.Linear(5 * 5 * 16, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool1(F.relu(self.conv1(x)))\n\n        x = self.pool2(F.relu(self.conv2(x)))\n\n        x = torch.flatten(x, 1)\n\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x","metadata":{"id":"lE8qJByKC-u0"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = LeNet().to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\n\nloaders = {\"train\": train_dataloader, \"valid\": valid_dataloader}","metadata":{"id":"dgfBX_HPyd_z"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Задание. Обучите CNN\nИспользуйте код обучения, который вы написали для полносвязной нейронной сети.","metadata":{"id":"J21MIKtjRNWA"}},{"cell_type":"code","source":"max_epochs = 10\naccuracy = {\"train\": [], \"valid\": []}\nfor epoch in range(max_epochs):\n    for k, dataloader in loaders.items():\n        epoch_correct = 0\n        epoch_all = 0\n        for x_batch, y_batch in dataloader:\n            if k == \"train\":\n                 model.train()\n                 optimizer.zero_grad()\n                 outp = model(x_batch)\n            else:\n                 model.eval()\n                 with torch.no_grad():\n                    outp = model(x_batch)\n\n            preds = outp.argmax(-1)\n            correct = (preds == y_batch).sum()\n            all = y_batch.size(0)\n            epoch_correct += correct.item()\n            epoch_all += all\n            if k == \"train\":\n                loss = criterion(outp, y_batch)\n                loss.backward()\n                optimizer.step()\n        if k == \"train\":\n            print(f\"Epoch: {epoch+1}\")\n\n        print(f\"Loader: {k}. Accuracy: {epoch_correct/epoch_all}\")\n        accuracy[k].append(epoch_correct/epoch_all)","metadata":{"id":"X06XkVKiFQLi","outputId":"b55a4a2d-84b1-4cbd-8229-1c596e0e369e"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lenet_accuracy = accuracy[\"valid\"]","metadata":{"id":"uFT5XKS1ytA2"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Сравним с предыдущем пунктом","metadata":{"id":"q-PwAI5SPMBx"}},{"cell_type":"code","source":"plt.figure(figsize=(16, 10))\nplt.title(\"Valid accuracy\")\nplt.plot(range(max_epochs), relu_accuracy, label=\"ReLU activation\", linewidth=2)\nplt.plot(range(max_epochs), leaky_relu_accuracy, label=\"LeakyReLU activation\", linewidth=2)\nplt.plot(range(max_epochs), elu_accuracy, label=\"ELU activation\", linewidth=2)\nplt.plot(range(max_epochs), lenet_accuracy, label=\"LeNet\", linewidth=2)\nplt.legend()\nplt.xlabel(\"Epoch\")\nplt.show()","metadata":{"id":"VDq6pSi_yytp","outputId":"becf5311-cdb0-4aca-8007-c3242b3ebf84"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Вопрос 6**\nКакое `accuracy` получается после обучения с точностью до двух знаков после запятой?\n\n**Ответ:** 0.99","metadata":{"id":"A3NCpTE3726m"}},{"cell_type":"code","source":"lenet_accuracy[-1]","metadata":{"id":"hrWQrE2mZo8-","outputId":"cab30ae1-dd1d-49b1-9442-534426dae25b"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"FPsINQS-Zro5"},"outputs":[],"execution_count":null}]}